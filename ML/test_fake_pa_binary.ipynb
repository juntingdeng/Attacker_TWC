{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juntingdeng/anaconda3/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import cupy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.utils.data as Data\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import csv\n",
    "import os\n",
    "from VAE import myVAE, myVAE2d, vaeTest, vaeGenerate, vaeGenerateMultiSigma, vaeGenerateSigma, myMemPolyVAE\n",
    "from classifier_cnn import Classifier, Classifier2D, addNoise2Batch\n",
    "from GAN_SDR import ClassifierSDR, Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierPABinary(nn.Module):\n",
    "    def __init__(self,nSDR):\n",
    "        super().__init__()\n",
    "\n",
    "        #For 800 length \n",
    "        self.conv0 = nn.Conv1d(in_channels=2, out_channels=32, kernel_size=5,stride=1, padding=2) # => 8*800\n",
    "        self.conv1 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=5,stride=1, padding=2) # => 8*800\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=8,stride=2, padding=3) # => 16*400\n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=6, stride=2, padding=2) # => 16*200\n",
    "\n",
    "        self.linear1 = nn.Linear(in_features=32*200, out_features=256)\n",
    "        self.linear2 = nn.Linear(in_features=256, out_features=nSDR)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        #For 800 length \n",
    "        x=F.relu(self.conv0(x))\n",
    "        x=F.relu(self.conv1(x))\n",
    "        x=F.relu(self.conv2(x))\n",
    "        x=F.relu(self.conv3(x))\n",
    "\n",
    "        # x=self.maxpool1(x)\n",
    "        flatten = nn.Flatten()\n",
    "        x=flatten(x)\n",
    "        x=F.relu(self.linear1(x))\n",
    "        x=self.linear2(x)\n",
    "        # x=F.sigmoid(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(nSDR, net, n_epochs, lr, batchsize,trainLoader, valLoader, testLoader,device, snr, noise):\n",
    "    n_epochs=n_epochs\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    trainLoss_curve=[]\n",
    "    trainAcc_curve=[]\n",
    "    trainAccDict_curve=[]\n",
    "    valLoss_curve=[]\n",
    "    valAcc_curve=[]\n",
    "    valAccDict_curve=[]\n",
    "    for epoch in range(n_epochs):\n",
    "        # print(\"epoch: \", epoch)\n",
    "        net.train()\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        train_acc_dict = torch.zeros((nSDR,2)).to(device)\n",
    "        val_loss = 0\n",
    "        val_acc=0\n",
    "        val_acc_dict = torch.zeros((nSDR,2)).to(device)\n",
    "        loss = None\n",
    "        for batch_idx, (data, labels) in enumerate(trainLoader):\n",
    "            data, labels = data.to(device, dtype=torch.float), labels.to(device,  dtype=torch.int64)\n",
    "            optimizer.zero_grad()\n",
    "            labels_ = net.forward(data)\n",
    "            loss = F.cross_entropy(labels_, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss+=loss.item()\n",
    "            labels_ = torch.argmax(labels_, dim=1)\n",
    "            train_acc+=(sum(labels_==labels)/batchsize)\n",
    "            for i in range(batchsize):\n",
    "                label=labels[i]\n",
    "                train_acc_dict[label][0]+=1\n",
    "                if labels_[i]==label:\n",
    "                    train_acc_dict[label][1]+=1\n",
    "            \n",
    "\n",
    "        for batch_idx, (data, labels) in enumerate(valLoader):\n",
    "            data, labels = data.to(device, dtype=torch.float), labels.to(device,  dtype=torch.int64)\n",
    "            optimizer.zero_grad()\n",
    "            labels_ = net.forward(data)\n",
    "            loss = F.cross_entropy(labels_, labels)\n",
    "            val_loss+=loss.item()\n",
    "            labels_ = torch.argmax(labels_, dim=1)\n",
    "            val_acc+=(sum(labels_==labels)/batchsize)\n",
    "            for i in range(batchsize):\n",
    "                label=labels[i]\n",
    "                val_acc_dict[label][0]+=1\n",
    "                if labels_[i]==label:\n",
    "                    val_acc_dict[label][1]+=1\n",
    "\n",
    "        train_loss = train_loss/len(trainLoader)\n",
    "        trainLoss_curve.append(train_loss)\n",
    "        train_acc = train_acc/len(trainLoader)\n",
    "        trainAcc_curve.append(train_acc.item())\n",
    "\n",
    "        val_loss = val_loss/len(valLoader)\n",
    "        valLoss_curve.append(val_loss)\n",
    "        val_acc = val_acc/len(valLoader)\n",
    "        valAcc_curve.append(val_acc.item())\n",
    "        \n",
    "        train_acc_spt = torch.round(train_acc_dict[:,1]/train_acc_dict[:,0], decimals=4)\n",
    "        val_acc_spt = torch.round(val_acc_dict[:,1]/val_acc_dict[:,0], decimals=4)\n",
    "        if (epoch+1)%10==0:\n",
    "            print('\\nEpoch: {} \\tTraining Loss: {:.6f}, Training Acc: {:.6f}'.format(epoch, train_loss, train_acc))\n",
    "            print('Epoch: {} \\tValidation Loss: {:.6f}, Validation Acc: {:.6f}'.format(epoch, val_loss, val_acc))\n",
    "            print(\"Train acc_dic \\t{}\".format(train_acc_spt.detach().cpu().numpy()))\n",
    "            print(\"Val acc_dic \\t{}\".format(val_acc_spt.detach().cpu().numpy()))\n",
    "            checkpoint_path=\"../../../RFF-RL/Code/checkpoint/Classifier/binaryPA_epoch\"+str(epoch+1)+\".cnn\"\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                'lr': lr,\n",
    "                'batch_size': batchsize,\n",
    "                'optimizer': optimizer,\n",
    "                'classifier': net,\n",
    "                'train_acc': train_acc,\n",
    "                'val_acc': val_acc,\n",
    "            }, checkpoint_path)\n",
    "\n",
    "    # test\n",
    "    test_acc=0\n",
    "    test_acc_dict = torch.zeros((nSDR,2)).to(device)\n",
    "    for batch_idx, (data, labels) in enumerate(testLoader):\n",
    "        data, labels = data.to(device, dtype=torch.float), labels.to(device,  dtype=torch.int64)\n",
    "        optimizer.zero_grad()\n",
    "        labels_ = net.forward(data)\n",
    "        labels_ = torch.argmax(labels_, dim=1)\n",
    "        test_acc+=(sum(labels_==labels)/batchsize)\n",
    "        for i in range(batchsize):\n",
    "            label=labels[i]\n",
    "            test_acc_dict[label][0]+=1\n",
    "            if labels_[i]==label:\n",
    "                test_acc_dict[label][1]+=1\n",
    "\n",
    "    test_acc = test_acc/len(testLoader)\n",
    "\n",
    "    test_acc_spt = torch.round(test_acc_dict[:,1]/test_acc_dict[:,0], decimals=4)\n",
    "    print('Test Acc: {:.6f}'.format(test_acc))\n",
    "    print(\"Test acc_dic \\t{}\".format(test_acc_spt.detach().cpu().numpy()))\n",
    "\n",
    "    # if save model\n",
    "    if not noise:\n",
    "        checkpoint_path=\"../../../RFF-RL/Code/checkpoint/Classifier/11SDR_nonoise_binaryPA_epoch\"+str(n_epochs)+\".cnn\"\n",
    "    else:\n",
    "        checkpoint_path=\"../../../RFF-RL/Code/checkpoint/Classifier/11SDR_binaryPA_epoch\"+str(n_epochs)+\".cnn\"\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        'lr': lr,\n",
    "        'batch_size': batchsize,\n",
    "        'optimizer': optimizer,\n",
    "        'classifier': net,\n",
    "        'train_acc': train_acc,\n",
    "        'val_acc': val_acc,\n",
    "        'test_acc': test_acc \n",
    "    }, checkpoint_path)\n",
    "    \n",
    "\n",
    "    train_acc_spt = train_acc_spt.detach().cpu().numpy()\n",
    "    val_acc_spt = val_acc_spt.detach().cpu().numpy()\n",
    "    test_acc_spt = test_acc_spt.detach().cpu().numpy()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(trainLoss_curve, label=\"Train Loss\")\n",
    "    plt.plot(valLoss_curve, label=\"Val Loss\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(trainAcc_curve, label=\"Train\")\n",
    "    plt.plot(valAcc_curve, label=\"Val\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy vs Epochs\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_acc_spt, label=\"Train Acc {:.2f}%\".format(train_acc*100))\n",
    "    plt.plot(val_acc_spt, label=\"Validation Acc {:.2f}%\".format(val_acc*100))\n",
    "    plt.plot(test_acc_spt, label=\"Test Acc {:.2f}%\".format(test_acc*100))\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"RFF\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy vs RFFs\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n",
      "noise = False\n",
      "datasets_total shape:  (132000, 2, 800)\n",
      "targets_total:  (132000,)\n",
      "datasets range:  tensor(1., dtype=torch.float64) tensor(-1., dtype=torch.float64)\n",
      "targets range:  tensor(1., dtype=torch.float64) tensor(0., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "## train binary classifier at 0.25 meters\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device: \", device)\n",
    "PATH=\"../../../RFF-RL/TcasRFFData/\"\n",
    "dataFile=\"ChipCAIRprocessedType3BlockOS20len800comb220V1p2Quant16.npz\"\n",
    "file = np.load(PATH+dataFile)\n",
    "items = file.files\n",
    "items = file.files\n",
    "datasets_all, targets_all = file[items[0]], file[items[1]]\n",
    "size=300\n",
    "datasets=np.zeros((220*size, 2, 800))\n",
    "targets=np.ones((220*size,))\n",
    "for i in range(220):\n",
    "    datasets[i*size:(i+1)*size] = datasets_all[i*730:i*730+size]\n",
    "\n",
    "\n",
    "noise = False\n",
    "ray=True\n",
    "snr = 35\n",
    "print(\"noise =\", noise)\n",
    "if noise:\n",
    "    print(\"snr = \", snr)\n",
    "    datasets_noised = addNoise2Batch(datasets, snr, device) #np array\n",
    "    datasets_noised = (datasets_noised - np.min(datasets_noised))/(np.max(datasets_noised) - np.min(datasets_noised)) # ~[0,1]\n",
    "    datasets_noised = 2*datasets_noised-1\n",
    "    print(np.max(datasets_noised))\n",
    "    print(np.min(datasets_noised))\n",
    "    # datasets=np.expand_dims(datasets, axis=1) # add one dimension for harmonics at channel 1\n",
    "    datasets_total = datasets_noised\n",
    "    targets_total = targets\n",
    "    \n",
    "elif ray:\n",
    "    h = np.random.rayleigh(1,(datasets.shape[0],1,800))\n",
    "    datasets_ray=h*datasets\n",
    "    datasets_ray = (datasets_ray - np.min(datasets_ray))/(np.max(datasets_ray) - np.min(datasets_ray)) # ~[0,1]\n",
    "    datasets_ray = 2*datasets_ray-1\n",
    "    datasets_total = np.concatenate((datasets,datasets_ray), axis=0)\n",
    "    targets_total = np.concatenate((np.ones(datasets.shape[0]),np.zeros(datasets.shape[0])), axis=0)\n",
    "    \n",
    "else:\n",
    "    datasets_total, targets_total = datasets, targets\n",
    "print(\"datasets_total shape: \", datasets_total.shape)\n",
    "print(\"targets_total: \", targets_total.shape)\n",
    "np.random.seed(0)\n",
    "datasets, targets = torch.tensor(datasets_total), torch.tensor(targets_total)\n",
    "print(\"datasets range: \", datasets.max(), datasets.min())\n",
    "print(\"targets range: \", targets.max(), targets.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 9 \tTraining Loss: 0.000000, Training Acc: 1.000000\n",
      "Epoch: 9 \tValidation Loss: 0.000000, Validation Acc: 1.000000\n",
      "Train acc_dic \t[1. 1.]\n",
      "Val acc_dic \t[1. 1.]\n",
      "\n",
      "Epoch: 19 \tTraining Loss: 0.000000, Training Acc: 1.000000\n",
      "Epoch: 19 \tValidation Loss: 0.000000, Validation Acc: 1.000000\n",
      "Train acc_dic \t[1. 1.]\n",
      "Val acc_dic \t[1. 1.]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mg:\\My Drive\\CMU\\Research\\attacker\\code\\ML\\test_fake_pa_binary.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/CMU/Research/attacker/code/ML/test_fake_pa_binary.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m n_harmonics\u001b[39m=\u001b[39mdatasets\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/CMU/Research/attacker/code/ML/test_fake_pa_binary.ipynb#W4sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m net \u001b[39m=\u001b[39m ClassifierPABinary(nSDR\u001b[39m=\u001b[39mbitarget)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/My%20Drive/CMU/Research/attacker/code/ML/test_fake_pa_binary.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m train(net\u001b[39m=\u001b[39;49mnet, nSDR\u001b[39m=\u001b[39;49mbitarget, n_epochs\u001b[39m=\u001b[39;49mn_epochs, lr\u001b[39m=\u001b[39;49mlr, batchsize\u001b[39m=\u001b[39;49mbatch_size,trainLoader\u001b[39m=\u001b[39;49mtrainLoader, valLoader\u001b[39m=\u001b[39;49mvalLoader, testLoader\u001b[39m=\u001b[39;49mtestLoader,device\u001b[39m=\u001b[39;49mdevice, snr \u001b[39m=\u001b[39;49m snr, noise\u001b[39m=\u001b[39;49mnoise)\n",
      "\u001b[1;32mg:\\My Drive\\CMU\\Research\\attacker\\code\\ML\\test_fake_pa_binary.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/CMU/Research/attacker/code/ML/test_fake_pa_binary.ipynb#W4sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m labels_ \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39mforward(data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/CMU/Research/attacker/code/ML/test_fake_pa_binary.ipynb#W4sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(labels_, labels)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/My%20Drive/CMU/Research/attacker/code/ML/test_fake_pa_binary.ipynb#W4sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/CMU/Research/attacker/code/ML/test_fake_pa_binary.ipynb#W4sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/CMU/Research/attacker/code/ML/test_fake_pa_binary.ipynb#W4sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m train_loss\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mloss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32md:\\Installation\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32md:\\Installation\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "datasets, targets = torch.tensor(datasets_total), torch.tensor(targets_total)\n",
    "# targets = targets.type(torch.DoubleTensor)\n",
    "SDRdatasets=Data.TensorDataset(datasets, targets)\n",
    "train_set, val_set, test_set = Data.random_split(SDRdatasets, [0.6, 0.2, 0.2])\n",
    "\n",
    "batch_size = 32\n",
    "n_epochs = 2000\n",
    "lr= 1e-3   #1e-3\n",
    "bitarget=2\n",
    "trainLoader = Data.DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "valLoader = Data.DataLoader(val_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "testLoader = Data.DataLoader(test_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "# testLoader = Data.DataLoader(rffTest, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "n_harmonics=datasets.shape[1]\n",
    "\n",
    "net = ClassifierPABinary(nSDR=bitarget).to(device)\n",
    "train(net=net, nSDR=bitarget, n_epochs=n_epochs, lr=lr, batchsize=batch_size,trainLoader=trainLoader, valLoader=valLoader, testLoader=testLoader,device=device, snr = snr, noise=noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "idx:0, RFF:0, snr:35, tran_acc:0.5, val_acc:1.0\n",
      "data shape: (500, 2, 800)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juntingd\\AppData\\Local\\Temp\\ipykernel_12180\\2205379270.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  datasets_total_fake=torch.tensor(datasets_total_fake, dtype=float).to(device)\n",
      "C:\\Users\\juntingd\\AppData\\Local\\Temp\\ipykernel_12180\\2205379270.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets_total_fake=torch.tensor(targets_total_fake, dtype=torch.int64).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "idx:1, RFF:1, snr:35, tran_acc:0.5938, val_acc:1.0\n",
      "data shape: (500, 2, 800)\n",
      "\n",
      "idx:2, RFF:4, snr:35, tran_acc:0.0781, val_acc:1.0\n",
      "data shape: (500, 2, 800)\n",
      "\n",
      "idx:3, RFF:34, snr:35, tran_acc:0.9883, val_acc:1.0\n",
      "data shape: (500, 2, 800)\n",
      "\n",
      "idx:4, RFF:45, snr:35, tran_acc:0.7656, val_acc:0.9375\n",
      "data shape: (500, 2, 800)\n",
      "\n",
      "idx:5, RFF:46, snr:35, tran_acc:0.5352, val_acc:1.0\n",
      "data shape: (500, 2, 800)\n",
      "\n",
      "idx:6, RFF:49, snr:35, tran_acc:0.6328, val_acc:0.9102\n",
      "data shape: (500, 2, 800)\n",
      "\n",
      "idx:7, RFF:52, snr:35, tran_acc:0.8906, val_acc:0.9102\n",
      "data shape: (500, 2, 800)\n",
      "\n",
      "idx:8, RFF:84, snr:35, tran_acc:0.5, val_acc:1.0\n",
      "data shape: (500, 2, 800)\n",
      "\n",
      "idx:9, RFF:108, snr:35, tran_acc:0.5078, val_acc:1.0\n",
      "data shape: (500, 2, 800)\n",
      "\n",
      "idx:10, RFF:171, snr:35, tran_acc:0.5, val_acc:1.0\n",
      "data shape: (500, 2, 800)\n",
      "\n",
      "idx:11, RFF:202, snr:35, tran_acc:0.2227, val_acc:0.9453\n",
      "data shape: (500, 2, 800)\n",
      "\n",
      "idx:12, RFF:205, snr:35, tran_acc:0.3633, val_acc:1.0\n",
      "data shape: (500, 2, 800)\n",
      "datasets:  torch.Size([6500, 2, 800])\n",
      "targets:  torch.Size([6500])\n",
      "datasets range:  tensor(1.2564, device='cuda:0', dtype=torch.float64) tensor(-1.0820, device='cuda:0', dtype=torch.float64)\n",
      "targets range:  tensor(0, device='cuda:0') tensor(0, device='cuda:0')\n",
      "test acc:0.30859375, FPR: 0.69140625\n",
      "test_acc_spt:  tensor([0.3086,    nan], device='cuda:0')\n",
      "sumi:6400.0\n",
      "sumi:0.0\n",
      "Wrong! tensor(nan)\n",
      "test_acc_spt_matix:  tensor([0.3086,    nan])\n"
     ]
    }
   ],
   "source": [
    "## test fake signal one time all on binary cnn\n",
    "device='cuda'\n",
    "nSDR=2\n",
    "samples_perSDR = 500\n",
    "classifier = ClassifierPABinary(nSDR=nSDR)\n",
    "clf_load=torch.load(\"../../../RFF-RL/Code/checkpoint/Classifier/binaryPA_epoch20.cnn\", map_location=device)\n",
    "# clf_load=torch.load(\"./checkpoint/ClassifierSDR/11SDR_normseperate_epoch2000.cnn\", map_location=device) \n",
    "classifier=clf_load[\"classifier\"]\n",
    "\n",
    "# datasets_total_fake = datasets_total[:][:][:]\n",
    "# targets_total_fake = targets_total[:]\n",
    "datasets_total_fake = np.zeros((6500, 2, 800))\n",
    "targets_total_fake = np.zeros((6500, ))\n",
    "\n",
    "path = \"./checkpoint/gan/gan_pa_all/snr35/\"\n",
    "ckps = os.listdir(path)\n",
    "batchsize=128\n",
    "idx=-1\n",
    "for ckp in ckps:\n",
    "    dataset_fake = torch.zeros((samples_perSDR, 2, 800))\n",
    "    datasets_total_fake=torch.tensor(datasets_total_fake, dtype=float).to(device)\n",
    "    targets_total_fake=torch.tensor(targets_total_fake, dtype=torch.int64).to(device)\n",
    "\n",
    "    if len(ckp.split(\"_\"))<2:\n",
    "        continue\n",
    "    idx+=1\n",
    "    RFFlabel = int(ckp.split(\"_\")[0][3:])\n",
    "    SNR = int(ckp.split(\"_\")[1][3:])\n",
    "    train_acc = float(ckp.split(\"_\")[3][8:])\n",
    "    val_acc = float(ckp.split(\"_\")[4][6:])\n",
    "    print(\"\\nidx:{}, RFF:{}, snr:{}, tran_acc:{}, val_acc:{}\".format(idx, RFFlabel, SNR, train_acc, val_acc))\n",
    "    ckp_load=torch.load(path+ckp, map_location=device)\n",
    "    generator = ckp_load[\"generator\"]\n",
    "    discriminator = ckp_load[\"discriminator\"]\n",
    "    classifier_load = ckp_load[\"classifier\"]\n",
    "\n",
    "    # data = data[:samples_perSDR]\n",
    "    data=datasets_all[RFFlabel*730:RFFlabel*730+samples_perSDR]\n",
    "    print(\"data shape:\", data.shape)\n",
    "    for sigIdx in range(data.shape[0]):\n",
    "        data[sigIdx]=-1+2*((data[sigIdx] - data[sigIdx].min())/(data[sigIdx].max() - data[sigIdx].min()))\n",
    "    data = addNoise2Batch(data, snr=SNR, device=device)\n",
    "    data = torch.tensor(data, dtype=torch.float, device=device)\n",
    "    \n",
    "    realdata_loader = Data.DataLoader(data, batch_size=batchsize, shuffle=True, drop_last=True)\n",
    "    for batch_idx, (data_batch) in enumerate(realdata_loader): \n",
    "        dataset_fake[batch_idx*batchsize:(batch_idx+1)*batchsize],mu, logvar = generator.forward(data_batch)\n",
    "    if (batch_idx+1)*batchsize<samples_perSDR:\n",
    "        repeat = samples_perSDR-(batch_idx+1)*batchsize\n",
    "        dataset_fake[-repeat:]=dataset_fake[:repeat]\n",
    "\n",
    "    datasets_total_fake[idx*samples_perSDR:(idx+1)*samples_perSDR]=dataset_fake\n",
    "    # targets_total_fake[SDRlabel*samples_perSDR:(SDRlabel+1)*samples_perSDR]=np.zeros(samples_perSDR)\n",
    "\n",
    "print(\"datasets: \", datasets_total_fake.shape)\n",
    "print(\"targets: \", targets_total_fake.shape)\n",
    "print(\"datasets range: \", datasets_total_fake.max(), datasets_total_fake.min())\n",
    "print(\"targets range: \", targets_total_fake.max(), targets_total_fake.min())\n",
    "# targets_total_fake = targets_total_fake.reshape((-1,1))\n",
    "\n",
    "testdatasets_fake=Data.TensorDataset(datasets_total_fake, targets_total_fake)\n",
    "testLoader_fake = Data.DataLoader(testdatasets_fake, batch_size=batchsize, shuffle=True, drop_last=True)\n",
    "\n",
    "test_acc=0\n",
    "fpr=0\n",
    "test_acc_dict = torch.zeros((nSDR,2)).to(device)\n",
    "test_acc_matrix = torch.zeros((nSDR, nSDR))\n",
    "for batch_idx, (data, labels) in enumerate(testLoader_fake):\n",
    "    data, labels = data.to(device, dtype=torch.float), labels.to(device,  dtype=torch.int64)\n",
    "    labels_ = classifier.forward(data)\n",
    "    labels_ = torch.argmax(labels_, dim=1)\n",
    "    test_acc+=(sum(labels_==labels)/batchsize)\n",
    "    fpr +=(sum(labels_!=labels)/batchsize)\n",
    "    for i in range(batchsize):\n",
    "        label=labels[i]\n",
    "        test_acc_matrix[label][labels_[i]]+=1\n",
    "        test_acc_dict[label][0]+=1\n",
    "        if labels_[i]==label:\n",
    "            test_acc_dict[label][1]+=1\n",
    "\n",
    "test_acc = test_acc/len(testLoader_fake)\n",
    "fpr = fpr/len(testLoader_fake)\n",
    "print(\"test acc:{}, FPR: {}\".format(test_acc, fpr))\n",
    "test_acc_spt = torch.round(test_acc_dict[:,1]/test_acc_dict[:,0], decimals=4)\n",
    "print(\"test_acc_spt: \", test_acc_spt)\n",
    "test_acc_spt_matrix = torch.zeros((nSDR)) \n",
    "for i in range(nSDR):\n",
    "    sumi = sum(test_acc_matrix[i])\n",
    "    print(\"sumi:{}\".format(sumi))\n",
    "    for j in range(nSDR):\n",
    "        test_acc_matrix[i][j] = test_acc_matrix[i][j]/sumi\n",
    "    test_acc_spt_matrix[i] = test_acc_matrix[i][i]\n",
    "    if sum(test_acc_matrix[i])!=1:\n",
    "        print(\"Wrong!\", sum(test_acc_matrix[i]))\n",
    "print(\"test_acc_spt_matix: \", test_acc_spt_matrix)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------snr35----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juntingd\\AppData\\Local\\Temp\\ipykernel_12180\\2223385192.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets_total_fake=torch.tensor(torch.ones(targets_all.shape[0]), dtype=torch.int64).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "idx:0, sdr:0, snr:35, tran_acc:0.5, val_acc:1.0\n",
      "data shape: (730, 2, 800)\n",
      "datasets:  torch.Size([160600, 2, 800])\n",
      "targets:  torch.Size([160600])\n",
      "datasets range:  tensor(1.1316, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>) tensor(-1.0362, device='cuda:0', dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "targets range:  tensor(1, device='cuda:0') tensor(0, device='cuda:0')\n",
      "test acc:0.9954645037651062\n",
      "test_acc_spt:  tensor([0., 1.], device='cuda:0')\n",
      "fake pos: tensor([  0., 728.])\n",
      "sumi:728.0\n",
      "sumi:159784.0\n",
      "test_acc_spt_matix:tensor([0., 1.]), FPR:1.0 \n",
      "\n",
      "idx:1, sdr:1, snr:35, tran_acc:0.5938, val_acc:1.0\n",
      "data shape: (730, 2, 800)\n",
      "datasets:  torch.Size([160600, 2, 800])\n",
      "targets:  torch.Size([160600])\n",
      "datasets range:  tensor(1.0622, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>) tensor(-1.0276, device='cuda:0', dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "targets range:  tensor(1, device='cuda:0') tensor(0, device='cuda:0')\n",
      "test acc:0.9954582452774048\n",
      "test_acc_spt:  tensor([0., 1.], device='cuda:0')\n",
      "fake pos: tensor([  0., 729.])\n",
      "sumi:729.0\n",
      "sumi:159783.0\n",
      "test_acc_spt_matix:tensor([0., 1.]), FPR:1.0 \n",
      "\n",
      "idx:2, sdr:4, snr:35, tran_acc:0.0781, val_acc:1.0\n",
      "data shape: (730, 2, 800)\n",
      "datasets:  torch.Size([160600, 2, 800])\n",
      "targets:  torch.Size([160600])\n",
      "datasets range:  tensor(1.2234, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>) tensor(-1., device='cuda:0', dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "targets range:  tensor(1, device='cuda:0') tensor(0, device='cuda:0')\n",
      "test acc:1.0\n",
      "test_acc_spt:  tensor([1., 1.], device='cuda:0')\n",
      "fake pos: tensor([730.,   0.])\n",
      "sumi:730.0\n",
      "sumi:159782.0\n",
      "test_acc_spt_matix:tensor([1., 1.]), FPR:0.0 \n",
      "\n",
      "idx:3, sdr:34, snr:35, tran_acc:0.9883, val_acc:1.0\n",
      "data shape: (730, 2, 800)\n",
      "datasets:  torch.Size([160600, 2, 800])\n",
      "targets:  torch.Size([160600])\n",
      "datasets range:  tensor(1., device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>) tensor(-1., device='cuda:0', dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "targets range:  tensor(1, device='cuda:0') tensor(0, device='cuda:0')\n",
      "test acc:1.0\n",
      "test_acc_spt:  tensor([1., 1.], device='cuda:0')\n",
      "fake pos: tensor([730.,   0.])\n",
      "sumi:730.0\n",
      "sumi:159782.0\n",
      "test_acc_spt_matix:tensor([1., 1.]), FPR:0.0 \n",
      "\n",
      "idx:4, sdr:45, snr:35, tran_acc:0.7656, val_acc:0.9375\n",
      "data shape: (730, 2, 800)\n",
      "datasets:  torch.Size([160600, 2, 800])\n",
      "targets:  torch.Size([160600])\n",
      "datasets range:  tensor(1.0166, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>) tensor(-1., device='cuda:0', dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "targets range:  tensor(1, device='cuda:0') tensor(0, device='cuda:0')\n",
      "test acc:0.9954582452774048\n",
      "test_acc_spt:  tensor([0., 1.], device='cuda:0')\n",
      "fake pos: tensor([  0., 729.])\n",
      "sumi:729.0\n",
      "sumi:159783.0\n",
      "test_acc_spt_matix:tensor([0., 1.]), FPR:1.0 \n",
      "\n",
      "idx:5, sdr:46, snr:35, tran_acc:0.5352, val_acc:1.0\n",
      "data shape: (730, 2, 800)\n",
      "datasets:  torch.Size([160600, 2, 800])\n",
      "targets:  torch.Size([160600])\n",
      "datasets range:  tensor(1.1032, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>) tensor(-1.0058, device='cuda:0', dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "targets range:  tensor(1, device='cuda:0') tensor(0, device='cuda:0')\n",
      "test acc:0.9954520463943481\n",
      "test_acc_spt:  tensor([0., 1.], device='cuda:0')\n",
      "fake pos: tensor([  0., 730.])\n",
      "sumi:730.0\n",
      "sumi:159782.0\n",
      "test_acc_spt_matix:tensor([0., 1.]), FPR:1.0 \n",
      "\n",
      "idx:6, sdr:49, snr:35, tran_acc:0.6328, val_acc:0.9102\n",
      "data shape: (730, 2, 800)\n",
      "datasets:  torch.Size([160600, 2, 800])\n",
      "targets:  torch.Size([160600])\n",
      "datasets range:  tensor(1.0442, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>) tensor(-1.0360, device='cuda:0', dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "targets range:  tensor(1, device='cuda:0') tensor(0, device='cuda:0')\n",
      "test acc:0.9954520463943481\n",
      "test_acc_spt:  tensor([0., 1.], device='cuda:0')\n",
      "fake pos: tensor([  0., 730.])\n",
      "sumi:730.0\n",
      "sumi:159782.0\n",
      "test_acc_spt_matix:tensor([0., 1.]), FPR:1.0 \n",
      "\n",
      "idx:7, sdr:52, snr:35, tran_acc:0.8906, val_acc:0.9102\n",
      "data shape: (730, 2, 800)\n",
      "datasets:  torch.Size([160600, 2, 800])\n",
      "targets:  torch.Size([160600])\n",
      "datasets range:  tensor(1.1758, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>) tensor(-1.0496, device='cuda:0', dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "targets range:  tensor(1, device='cuda:0') tensor(0, device='cuda:0')\n",
      "test acc:0.9954520463943481\n",
      "test_acc_spt:  tensor([0., 1.], device='cuda:0')\n",
      "fake pos: tensor([  0., 730.])\n",
      "sumi:730.0\n",
      "sumi:159782.0\n",
      "test_acc_spt_matix:tensor([0., 1.]), FPR:1.0 \n",
      "\n",
      "idx:8, sdr:84, snr:35, tran_acc:0.5, val_acc:1.0\n",
      "data shape: (730, 2, 800)\n",
      "datasets:  torch.Size([160600, 2, 800])\n",
      "targets:  torch.Size([160600])\n",
      "datasets range:  tensor(1.2007, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>) tensor(-1., device='cuda:0', dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "targets range:  tensor(1, device='cuda:0') tensor(0, device='cuda:0')\n",
      "test acc:1.0\n",
      "test_acc_spt:  tensor([1., 1.], device='cuda:0')\n",
      "fake pos: tensor([730.,   0.])\n",
      "sumi:730.0\n",
      "sumi:159782.0\n",
      "test_acc_spt_matix:tensor([1., 1.]), FPR:0.0 \n",
      "\n",
      "idx:9, sdr:108, snr:35, tran_acc:0.5078, val_acc:1.0\n",
      "data shape: (730, 2, 800)\n",
      "datasets:  torch.Size([160600, 2, 800])\n",
      "targets:  torch.Size([160600])\n",
      "datasets range:  tensor(1.2564, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>) tensor(-1.0820, device='cuda:0', dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "targets range:  tensor(1, device='cuda:0') tensor(0, device='cuda:0')\n",
      "test acc:0.9954520463943481\n",
      "test_acc_spt:  tensor([0., 1.], device='cuda:0')\n",
      "fake pos: tensor([  0., 730.])\n",
      "sumi:730.0\n",
      "sumi:159782.0\n",
      "test_acc_spt_matix:tensor([0., 1.]), FPR:1.0 \n",
      "\n",
      "idx:10, sdr:171, snr:35, tran_acc:0.5, val_acc:1.0\n",
      "data shape: (730, 2, 800)\n",
      "datasets:  torch.Size([160600, 2, 800])\n",
      "targets:  torch.Size([160600])\n",
      "datasets range:  tensor(1.0259, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>) tensor(-1.0720, device='cuda:0', dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "targets range:  tensor(1, device='cuda:0') tensor(0, device='cuda:0')\n",
      "test acc:0.9954520463943481\n",
      "test_acc_spt:  tensor([0., 1.], device='cuda:0')\n",
      "fake pos: tensor([  0., 730.])\n",
      "sumi:730.0\n",
      "sumi:159782.0\n",
      "test_acc_spt_matix:tensor([0., 1.]), FPR:1.0 \n",
      "\n",
      "idx:11, sdr:202, snr:35, tran_acc:0.2227, val_acc:0.9453\n",
      "data shape: (730, 2, 800)\n",
      "datasets:  torch.Size([160600, 2, 800])\n",
      "targets:  torch.Size([160600])\n",
      "datasets range:  tensor(1.2473, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>) tensor(-1., device='cuda:0', dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "targets range:  tensor(1, device='cuda:0') tensor(0, device='cuda:0')\n",
      "test acc:1.0\n",
      "test_acc_spt:  tensor([1., 1.], device='cuda:0')\n",
      "fake pos: tensor([730.,   0.])\n",
      "sumi:730.0\n",
      "sumi:159782.0\n",
      "test_acc_spt_matix:tensor([1., 1.]), FPR:0.0 \n",
      "\n",
      "idx:12, sdr:205, snr:35, tran_acc:0.3633, val_acc:1.0\n",
      "data shape: (730, 2, 800)\n",
      "datasets:  torch.Size([160600, 2, 800])\n",
      "targets:  torch.Size([160600])\n",
      "datasets range:  tensor(1.0573, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>) tensor(-1.0744, device='cuda:0', dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "targets range:  tensor(1, device='cuda:0') tensor(0, device='cuda:0')\n",
      "test acc:0.9954520463943481\n",
      "test_acc_spt:  tensor([0., 1.], device='cuda:0')\n",
      "fake pos: tensor([  0., 730.])\n",
      "sumi:730.0\n",
      "sumi:159782.0\n",
      "test_acc_spt_matix:tensor([0., 1.]), FPR:1.0 \n",
      "\n",
      "-----------------------snr5----------------------\n",
      "\n",
      "idx:0, sdr:0, snr:5, tran_acc:0.6797, val_acc:0.9414\n",
      "data shape: (730, 2, 800)\n",
      "datasets:  torch.Size([160600, 2, 800])\n",
      "targets:  torch.Size([160600])\n",
      "datasets range:  tensor(1.0567, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>) tensor(-1., device='cuda:0', dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "targets range:  tensor(1, device='cuda:0') tensor(0, device='cuda:0')\n",
      "test acc:1.0\n",
      "test_acc_spt:  tensor([1., 1.], device='cuda:0')\n",
      "fake pos: tensor([729.,   0.])\n",
      "sumi:729.0\n",
      "sumi:159783.0\n",
      "test_acc_spt_matix:tensor([1., 1.]), FPR:0.0 \n",
      "\n",
      "idx:1, sdr:1, snr:5, tran_acc:0.3438, val_acc:1.0\n",
      "data shape: (730, 2, 800)\n",
      "datasets:  torch.Size([160600, 2, 800])\n",
      "targets:  torch.Size([160600])\n",
      "datasets range:  tensor(1.2060, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>) tensor(-1., device='cuda:0', dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "targets range:  tensor(1, device='cuda:0') tensor(0, device='cuda:0')\n",
      "test acc:0.9954520463943481\n",
      "test_acc_spt:  tensor([0., 1.], device='cuda:0')\n",
      "fake pos: tensor([  0., 730.])\n",
      "sumi:730.0\n",
      "sumi:159782.0\n",
      "test_acc_spt_matix:tensor([0., 1.]), FPR:1.0 \n",
      "\n",
      "idx:2, sdr:4, snr:5, tran_acc:0.0234, val_acc:1.0\n",
      "data shape: (730, 2, 800)\n",
      "datasets:  torch.Size([160600, 2, 800])\n",
      "targets:  torch.Size([160600])\n",
      "datasets range:  tensor(1.1669, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>) tensor(-1., device='cuda:0', dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "targets range:  tensor(1, device='cuda:0') tensor(0, device='cuda:0')\n",
      "test acc:0.9954582452774048\n",
      "test_acc_spt:  tensor([0., 1.], device='cuda:0')\n",
      "fake pos: tensor([  0., 729.])\n",
      "sumi:729.0\n",
      "sumi:159783.0\n",
      "test_acc_spt_matix:tensor([0., 1.]), FPR:1.0 \n",
      "\n",
      "idx:3, sdr:34, snr:5, tran_acc:0.6055, val_acc:0.5195\n",
      "data shape: (730, 2, 800)\n",
      "datasets:  torch.Size([160600, 2, 800])\n",
      "targets:  torch.Size([160600])\n",
      "datasets range:  tensor(3.2595, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>) tensor(-2.1087, device='cuda:0', dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "targets range:  tensor(1, device='cuda:0') tensor(0, device='cuda:0')\n",
      "test acc:0.9954520463943481\n",
      "test_acc_spt:  tensor([0., 1.], device='cuda:0')\n",
      "fake pos: tensor([  0., 730.])\n",
      "sumi:730.0\n",
      "sumi:159782.0\n",
      "test_acc_spt_matix:tensor([0., 1.]), FPR:1.0 \n",
      "\n",
      "idx:4, sdr:45, snr:5, tran_acc:0.4336, val_acc:0.3555\n",
      "data shape: (730, 2, 800)\n",
      "datasets:  torch.Size([160600, 2, 800])\n",
      "targets:  torch.Size([160600])\n",
      "datasets range:  tensor(4.2516, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>) tensor(-2.2128, device='cuda:0', dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "targets range:  tensor(1, device='cuda:0') tensor(0, device='cuda:0')\n",
      "test acc:0.9954582452774048\n",
      "test_acc_spt:  tensor([0., 1.], device='cuda:0')\n",
      "fake pos: tensor([  0., 729.])\n",
      "sumi:729.0\n",
      "sumi:159783.0\n",
      "test_acc_spt_matix:tensor([0., 1.]), FPR:1.0 \n",
      "\n",
      "idx:5, sdr:46, snr:5, tran_acc:0.3828, val_acc:0.25\n",
      "data shape: (730, 2, 800)\n",
      "datasets:  torch.Size([160600, 2, 800])\n",
      "targets:  torch.Size([160600])\n",
      "datasets range:  tensor(3.3427, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>) tensor(-2.3870, device='cuda:0', dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "targets range:  tensor(1, device='cuda:0') tensor(0, device='cuda:0')\n",
      "test acc:0.9954520463943481\n",
      "test_acc_spt:  tensor([0., 1.], device='cuda:0')\n",
      "fake pos: tensor([  0., 730.])\n",
      "sumi:730.0\n",
      "sumi:159782.0\n",
      "test_acc_spt_matix:tensor([0., 1.]), FPR:1.0 \n",
      "\n",
      "idx:6, sdr:49, snr:5, tran_acc:0.9336, val_acc:0.8828\n",
      "data shape: (730, 2, 800)\n",
      "datasets:  torch.Size([160600, 2, 800])\n",
      "targets:  torch.Size([160600])\n",
      "datasets range:  tensor(1.0344, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>) tensor(-1.0140, device='cuda:0', dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "targets range:  tensor(1, device='cuda:0') tensor(0, device='cuda:0')\n",
      "test acc:0.9954520463943481\n",
      "test_acc_spt:  tensor([0., 1.], device='cuda:0')\n",
      "fake pos: tensor([  0., 730.])\n",
      "sumi:730.0\n",
      "sumi:159782.0\n",
      "test_acc_spt_matix:tensor([0., 1.]), FPR:1.0 \n",
      "\n",
      "idx:7, sdr:52, snr:5, tran_acc:0.0156, val_acc:0.0078\n",
      "data shape: (730, 2, 800)\n",
      "datasets:  torch.Size([160600, 2, 800])\n",
      "targets:  torch.Size([160600])\n",
      "datasets range:  tensor(3.2270, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>) tensor(-2.3215, device='cuda:0', dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "targets range:  tensor(1, device='cuda:0') tensor(0, device='cuda:0')\n",
      "test acc:0.9954582452774048\n",
      "test_acc_spt:  tensor([0., 1.], device='cuda:0')\n",
      "fake pos: tensor([  0., 729.])\n",
      "sumi:729.0\n",
      "sumi:159783.0\n",
      "test_acc_spt_matix:tensor([0., 1.]), FPR:1.0 \n",
      "\n",
      "idx:8, sdr:84, snr:5, tran_acc:0.0898, val_acc:0.0898\n",
      "data shape: (730, 2, 800)\n",
      "datasets:  torch.Size([160600, 2, 800])\n",
      "targets:  torch.Size([160600])\n",
      "datasets range:  tensor(2.5770, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>) tensor(-2.0117, device='cuda:0', dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "targets range:  tensor(1, device='cuda:0') tensor(0, device='cuda:0')\n",
      "test acc:0.9954520463943481\n",
      "test_acc_spt:  tensor([0., 1.], device='cuda:0')\n",
      "fake pos: tensor([  0., 730.])\n",
      "sumi:730.0\n",
      "sumi:159782.0\n",
      "test_acc_spt_matix:tensor([0., 1.]), FPR:1.0 \n",
      "\n",
      "idx:9, sdr:108, snr:5, tran_acc:0.0938, val_acc:0.0469\n",
      "data shape: (730, 2, 800)\n",
      "datasets:  torch.Size([160600, 2, 800])\n",
      "targets:  torch.Size([160600])\n",
      "datasets range:  tensor(3.6453, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>) tensor(-1.7804, device='cuda:0', dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "targets range:  tensor(1, device='cuda:0') tensor(0, device='cuda:0')\n",
      "test acc:0.9954520463943481\n",
      "test_acc_spt:  tensor([0., 1.], device='cuda:0')\n",
      "fake pos: tensor([  0., 730.])\n",
      "sumi:730.0\n",
      "sumi:159782.0\n",
      "test_acc_spt_matix:tensor([0., 1.]), FPR:1.0 \n",
      "\n",
      "idx:10, sdr:171, snr:5, tran_acc:0.6836, val_acc:1.0\n",
      "data shape: (730, 2, 800)\n",
      "datasets:  torch.Size([160600, 2, 800])\n",
      "targets:  torch.Size([160600])\n",
      "datasets range:  tensor(1.1541, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>) tensor(-1.0202, device='cuda:0', dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "targets range:  tensor(1, device='cuda:0') tensor(0, device='cuda:0')\n",
      "test acc:0.9954582452774048\n",
      "test_acc_spt:  tensor([0., 1.], device='cuda:0')\n",
      "fake pos: tensor([  0., 729.])\n",
      "sumi:729.0\n",
      "sumi:159783.0\n",
      "test_acc_spt_matix:tensor([0., 1.]), FPR:1.0 \n",
      "\n",
      "idx:11, sdr:202, snr:5, tran_acc:0.0, val_acc:0.0\n",
      "data shape: (730, 2, 800)\n",
      "datasets:  torch.Size([160600, 2, 800])\n",
      "targets:  torch.Size([160600])\n",
      "datasets range:  tensor(2.8426, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>) tensor(-1.8571, device='cuda:0', dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "targets range:  tensor(1, device='cuda:0') tensor(0, device='cuda:0')\n",
      "test acc:0.9954582452774048\n",
      "test_acc_spt:  tensor([0., 1.], device='cuda:0')\n",
      "fake pos: tensor([  0., 729.])\n",
      "sumi:729.0\n",
      "sumi:159783.0\n",
      "test_acc_spt_matix:tensor([0., 1.]), FPR:1.0 \n",
      "\n",
      "idx:12, sdr:205, snr:5, tran_acc:0.6094, val_acc:0.9297\n",
      "data shape: (730, 2, 800)\n",
      "datasets:  torch.Size([160600, 2, 800])\n",
      "targets:  torch.Size([160600])\n",
      "datasets range:  tensor(1.2616, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>) tensor(-1.0333, device='cuda:0', dtype=torch.float64, grad_fn=<MinBackward1>)\n",
      "targets range:  tensor(1, device='cuda:0') tensor(0, device='cuda:0')\n",
      "test acc:0.9954582452774048\n",
      "test_acc_spt:  tensor([0., 1.], device='cuda:0')\n",
      "fake pos: tensor([  0., 729.])\n",
      "sumi:729.0\n",
      "sumi:159783.0\n",
      "test_acc_spt_matix:tensor([0., 1.]), FPR:1.0 \n"
     ]
    }
   ],
   "source": [
    "## test fake signal one time one SDR on binary cnn\n",
    "device='cuda'\n",
    "\n",
    "nSDR=2\n",
    "samples_perSDR = 730\n",
    "classifier = ClassifierPABinary(nSDR=nSDR)\n",
    "clf_load=torch.load(\"../../../RFF-RL/Code/checkpoint/Classifier/binaryPA_epoch20.cnn\", map_location=device)\n",
    "# clf_load=torch.load(\"./checkpoint/ClassifierSDR/11SDR_normseperate_epoch2000.cnn\", map_location=device) \n",
    "classifier=clf_load[\"classifier\"]\n",
    "\n",
    "for snr in [35, 5]:\n",
    "    print(\"\\n-----------------------snr{}----------------------\".format(snr))\n",
    "    path = \"./checkpoint/gan/gan_pa_all/snr\"+str(snr)+\"/\"\n",
    "    ckps = os.listdir(path)\n",
    "    idx=-1\n",
    "    batchsize=128\n",
    "    for ckp in ckps:\n",
    "        dataset_fake = torch.zeros((samples_perSDR, 2, 800))\n",
    "        datasets_total_fake=torch.tensor(datasets_all[:][:][:], dtype=float).to(device)\n",
    "        targets_total_fake=torch.tensor(torch.ones(targets_all.shape[0]), dtype=torch.int64).to(device)\n",
    "\n",
    "        if len(ckp.split(\"_\"))<2:\n",
    "            continue\n",
    "        idx+=1\n",
    "        RFFlabel = int(ckp.split(\"_\")[0][3:])\n",
    "        SNR = int(ckp.split(\"_\")[1][3:])\n",
    "        train_acc = float(ckp.split(\"_\")[3][8:])\n",
    "        val_acc = float(ckp.split(\"_\")[4][6:])\n",
    "        print(\"\\nidx:{}, sdr:{}, snr:{}, tran_acc:{}, val_acc:{}\".format(idx, RFFlabel, SNR, train_acc, val_acc))\n",
    "        ckp_load=torch.load(path+ckp, map_location=device)\n",
    "        generator = ckp_load[\"generator\"]\n",
    "        discriminator = ckp_load[\"discriminator\"]\n",
    "        classifier_load = ckp_load[\"classifier\"]\n",
    "\n",
    "        data=datasets_all[RFFlabel*730:RFFlabel*730+samples_perSDR]\n",
    "        print(\"data shape:\", data.shape)\n",
    "        for sigIdx in range(data.shape[0]):\n",
    "            data[sigIdx]=-1+2*((data[sigIdx] - data[sigIdx].min())/(data[sigIdx].max() - data[sigIdx].min()))\n",
    "        data = addNoise2Batch(data, snr=SNR, device=device)\n",
    "        data = torch.tensor(data, dtype=torch.float, device=device)\n",
    "    \n",
    "        realdata_loader = Data.DataLoader(data, batch_size=batchsize, shuffle=True, drop_last=True)\n",
    "        for batch_idx, (data_batch) in enumerate(realdata_loader): \n",
    "            dataset_fake[batch_idx*batchsize:(batch_idx+1)*batchsize],mu, logvar = generator.forward(data_batch)\n",
    "        if (batch_idx+1)*batchsize<samples_perSDR:\n",
    "            repeat = samples_perSDR-(batch_idx+1)*batchsize\n",
    "            dataset_fake[-repeat:]=dataset_fake[:repeat]\n",
    "        \n",
    "        datasets_total_fake[RFFlabel*730:RFFlabel*730+samples_perSDR]=dataset_fake\n",
    "        targets_total_fake[RFFlabel*730:RFFlabel*730+samples_perSDR]=torch.zeros(samples_perSDR)\n",
    "\n",
    "        print(\"datasets: \", datasets_total_fake.shape)\n",
    "        print(\"targets: \", targets_total_fake.shape)\n",
    "        print(\"datasets range: \", datasets_total_fake.max(), datasets_total_fake.min())\n",
    "        print(\"targets range: \", targets_total_fake.max(), targets_total_fake.min())\n",
    "        # targets_total_fake = targets_total_fake.reshape((-1,1))\n",
    "\n",
    "        testdatasets_fake=Data.TensorDataset(datasets_total_fake, targets_total_fake)\n",
    "        testLoader_fake = Data.DataLoader(testdatasets_fake, batch_size=batchsize, shuffle=True, drop_last=True)\n",
    "\n",
    "        test_acc=0\n",
    "        fpr=0\n",
    "        test_acc_dict = torch.zeros((nSDR,2)).to(device)\n",
    "        test_acc_matrix = torch.zeros((nSDR, nSDR))\n",
    "        for batch_idx, (data, labels) in enumerate(testLoader_fake):\n",
    "            data, labels = data.to(device, dtype=torch.float), labels.to(device,  dtype=torch.int64)\n",
    "            labels_ = classifier.forward(data)\n",
    "            labels_ = torch.argmax(labels_, dim=1)\n",
    "            test_acc+=(sum(labels_==labels)/batchsize)\n",
    "            for i in range(batchsize):\n",
    "                label=labels[i]\n",
    "                test_acc_matrix[label][labels_[i]]+=1\n",
    "                test_acc_dict[label][0]+=1\n",
    "                if labels_[i]==label:\n",
    "                    test_acc_dict[label][1]+=1\n",
    "\n",
    "        test_acc = test_acc/len(testLoader_fake)\n",
    "        print(\"test acc:{}\".format(test_acc))\n",
    "        test_acc_spt = torch.round(test_acc_dict[:,1]/test_acc_dict[:,0], decimals=4)\n",
    "        print(\"test_acc_spt: \", test_acc_spt)\n",
    "        test_acc_spt_matrix = torch.zeros((nSDR)) \n",
    "        print(\"fake pos:\", test_acc_matrix[0])\n",
    "        for i in range(nSDR):\n",
    "            sumi = sum(test_acc_matrix[i])\n",
    "            \n",
    "            print(\"sumi:{}\".format(sumi))\n",
    "            for j in range(nSDR):\n",
    "                test_acc_matrix[i][j] = test_acc_matrix[i][j]/sumi\n",
    "            test_acc_spt_matrix[i] = test_acc_matrix[i][i]\n",
    "            if sum(test_acc_matrix[i])!=1:\n",
    "                print(\"Wrong!\", sum(test_acc_matrix[i]))\n",
    "        print(\"test_acc_spt_matix:{}, FPR:{} \".format(test_acc_spt_matrix, 1-test_acc_spt_matrix[0]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR:35, test acc:0.0,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_2/q6x8216d4lvb82hk5k20k1lh0000gn/T/ipykernel_49604/1108014849.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  datasets, targets = torch.tensor(datasets, dtype=torch.float).to(device), torch.tensor(targets, dtype=torch.int64).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR:30, test acc:0.0,\n",
      "SNR:25, test acc:0.0,\n",
      "SNR:20, test acc:0.0,\n",
      "SNR:15, test acc:0.0,\n",
      "SNR:10, test acc:0.0,\n",
      "SNR:5, test acc:0.0,\n"
     ]
    }
   ],
   "source": [
    "## test MC simulation\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "nSDR=2\n",
    "\n",
    "batchsize=32\n",
    "classifier = ClassifierPABinary(nSDR=nSDR)\n",
    "clf_load=torch.load(\"../../../RFF-RL/Code/checkpoint/Classifier/binaryPA_epoch10.cnn\", map_location=device)\n",
    "# clf_load=torch.load(\"./checkpoint/ClassifierSDR/11SDR_normseperate_epoch2000.cnn\", map_location=device) \n",
    "classifier=clf_load[\"classifier\"]\n",
    "\n",
    "\n",
    "datasets_mc=np.load(\"../../../RFF-RL/MCsim_for_generative_crosstest/MCSim_cubic_603_2_800.npy\")\n",
    "targets=np.zeros((datasets_mc.shape[0]))\n",
    "\n",
    "\n",
    "for SNR in [35,30,25,20,15,10,5]:\n",
    "    datasets = addNoise2Batch(datasets_mc, snr=SNR, device=device)\n",
    "    datasets, targets = torch.tensor(datasets, dtype=torch.float).to(device), torch.tensor(targets, dtype=torch.int64).to(device)\n",
    "    testdatasets=Data.TensorDataset(datasets, targets)\n",
    "    testLoader = Data.DataLoader(testdatasets, batch_size=batchsize, shuffle=True, drop_last=True)\n",
    "\n",
    "    test_acc=0\n",
    "    for batch_idx, (data, labels) in enumerate(testLoader):\n",
    "        data, labels = data.to(device, dtype=torch.float), labels.to(device,  dtype=torch.int64)\n",
    "        labels_ = classifier.forward(data)\n",
    "        labels_ = torch.argmax(labels_, dim=1)\n",
    "        test_acc+=(sum(labels_==labels)/batchsize)\n",
    "\n",
    "    print(\"SNR:{}, test acc:{},\".format(SNR, test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
